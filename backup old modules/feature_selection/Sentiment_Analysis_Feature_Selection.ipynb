{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_TXXxPmkgRW"
   },
   "source": [
    "# Sentiment Analysis - Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "F7MaW4TABbas",
    "outputId": "d1a47564-68d2-4f65-f2a0-cb8bc362b94c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /home/gemilang/.local/lib/python3.7/site-packages (3.3)\r\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/lib/python3.7/site-packages (from nltk) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myrfL7_zBXdC"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPRnAJxCdNut"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gemilang/.local/lib/python3.7/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "my_df_training = pd.read_csv('clean_tweet_training.csv',index_col=0)\n",
    "tweet_text_training = my_df_training['text']\n",
    "target_training = my_df_training['target']\n",
    "X_training = tweet_text_training[pd.notnull(tweet_text_training)]\n",
    "Y_training = target_training[pd.notnull(tweet_text_training)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7RQNAGvTWRn"
   },
   "source": [
    "Karena data training terlalu besar, kita akan menggunakan sebagian kecil dari data (1%) agar proses training lebih cepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ymVx8iM0gzy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gemilang/.local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_part1, X_train_part2, Y_train_part1, Y_test_part2 = train_test_split(X_training, Y_training, train_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOwkXJVxns6l"
   },
   "outputs": [],
   "source": [
    "my_df_test = pd.read_csv('clean_tweet_test_fixed.csv',index_col=0)\n",
    "my_df_test = my_df_test[my_df_test.target != 2]\n",
    "tweet_text_test = my_df_test['text']\n",
    "target_test = my_df_test['target']\n",
    "X_test = tweet_text_test[pd.notnull(tweet_text_test)]\n",
    "Y_test = target_test[pd.notnull(tweet_text_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f4uxTrVjkF3-"
   },
   "source": [
    "## Feature Extraction <br>\n",
    "Seperti pada notebook sebelumnya, kita menggunakan bag-of-words sebagai feature. Total ada 17489 terms (unique words) yang terindex dalam vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Tb27bzGXjwec",
    "outputId": "425c54c7-44ca-4e3a-8c51-3ec674b707d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  17631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_training_vector = vectorizer.fit_transform(X_train_part1)\n",
    "print(\"Number of features:  %d\" % len(vectorizer.vocabulary_))\n",
    "X_test_vector = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwHNArC1k1CX"
   },
   "source": [
    "Untuk melihat bagaimana performance/akurasi sistem dengan menggunakan BagOfWords sebagai feature, kita akan mengaplikasikan salah satu classifier model yaitu SVM (Support Vector Machine). <br>\n",
    "Dengan menggunakan 17489 features, SVM berhasil meng-klasifikasikan tweets dengan akurasi **72.70%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "VVZT0WvolGDB",
    "outputId": "0cc55220-b21f-4726-fa38-4e83d0fa00fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Predicting...\n",
      "Accuracy: 0.7409470752089137\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = svm.SVC(kernel='linear')\n",
    "print(\"Training Classifier...\")\n",
    "clf.fit(X_training_vector, Y_train_part1)\n",
    "print(\"Predicting...\")\n",
    "prediction = clf.predict(X_test_vector)\n",
    "accuracy = accuracy_score(Y_test, prediction)\n",
    "print ('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3kl14iu36t5"
   },
   "source": [
    "## (1) Frequency-based Feature Selection <br>\n",
    "Selanjutnya, kita akan mengaplikasikan feature selection method yang paling sederhana, yaitu frequency-based feature selection. <br>\n",
    "Kita akan menggunakan hanya 700 features yang frekuensi kemunculannya paling sering. <br>\n",
    "Dari list 10 kata yang paling sering muncul adalah *good, day, get, like, go, love, going, work, today, got* <br>\n",
    "Dengan 700 features, akurasi dari SVM dalam mengklasifikasikan tweets naik hingga **75.21%**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "tYiYrAZIoxar",
    "outputId": "831b45fc-f8fe-4ce3-cd59-ccd99f132102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  700\n",
      "------------------\n",
      "good 851\n",
      "day 851\n",
      "get 817\n",
      "like 754\n",
      "go 748\n",
      "today 663\n",
      "going 650\n",
      "love 628\n",
      "work 614\n",
      "back 613\n",
      "------------------\n",
      "Training Classifier...\n",
      "Predicting...\n",
      "Accuracy: 0.7298050139275766\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=700)\n",
    "X_training_vector = vectorizer.fit_transform(X_train_part1)\n",
    "print(\"Number of features:  %d\" % len(vectorizer.vocabulary_))\n",
    "print(\"------------------\")\n",
    "\n",
    "sum_words = X_training_vector.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "most_freq = words_freq[:10]\n",
    "for word, freq in most_freq:\n",
    "  print(word, freq)\n",
    "\n",
    "X_test_vector = vectorizer.transform(X_test)\n",
    "print(\"------------------\")\n",
    "print(\"Training Classifier...\")\n",
    "clf.fit(X_training_vector, Y_train_part1)\n",
    "print(\"Predicting...\")\n",
    "prediction = clf.predict(X_test_vector)\n",
    "accuracy = accuracy_score(Y_test, prediction)\n",
    "print ('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-IyB-UH15pN2"
   },
   "source": [
    "## (2) Select K-Best Features Selection dengan Chi2 <br>\n",
    "Selanjutnya, kita akan menggunakan chi2 sebagai parameter untuk menentukan K-Best features. <br>\n",
    "Sama seperti sebelumnya, hanya akan digunakan 700 features. Dengan menggunakan Chi2, 10 terms yang paling signifikan adalah *sad, thanks, miss, love, good, thank, sick, work, bad, hate* .<br>\n",
    "Akurasi meningkat hingga **78.27%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "d9rE_g6Z5sJ_",
    "outputId": "07b0aafd-8a83-44e5-bb79-e4d25f19c0e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  17631\n",
      "---------------------\n",
      "sad 238.54023574959746\n",
      "thanks 209.71485062050235\n",
      "miss 168.3154279421448\n",
      "love 159.80138255363354\n",
      "sick 133.3229217791157\n",
      "good 131.14237035467895\n",
      "bad 112.47065955957137\n",
      "wish 108.39876788533053\n",
      "sorry 105.39613173269423\n",
      "thank 102.13420300182054\n",
      "---------------------\n",
      "Training Classifier...\n",
      "Predicting...\n",
      "Accuracy: 0.7437325905292479\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_training_vector = vectorizer.fit_transform(X_train_part1)\n",
    "print(\"Number of features:  %d\" % len(vectorizer.vocabulary_))\n",
    "print (\"---------------------\")\n",
    "X_test_vector = vectorizer.transform(X_test)\n",
    "\n",
    "#print features their chi2 score\n",
    "feature_scores = chi2(X_training_vector, Y_train_part1)[0]\n",
    "for score, fname in sorted(zip(feature_scores, vectorizer.get_feature_names()), reverse=True)[:10]:\n",
    "    print(fname, score)\n",
    "\n",
    "#selectKBest feature using Chi2 and see whether it could improve the accuracy\n",
    "ch2 = SelectKBest(chi2, k=700)\n",
    "X_train_best = ch2.fit_transform(X_training_vector, Y_train_part1)\n",
    "X_test_best = ch2.transform(X_test_vector)\n",
    "print (\"---------------------\")\n",
    "print(\"Training Classifier...\")\n",
    "clf.fit(X_train_best, Y_train_part1)\n",
    "print(\"Predicting...\")\n",
    "prediction = clf.predict(X_test_best)\n",
    "accuracy = accuracy_score(Y_test, prediction)\n",
    "print ('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q1SsNN1xEt2D"
   },
   "source": [
    "## (3) Select K-Best Features Selection dengan Mutual Information <br>\n",
    "Selanjutnya, kita akan menggunakan Mutual Information (MI) sebagai parameter untuk menentukan K-Best features. <br>\n",
    "Sama seperti sebelumnya, hanya akan digunakan 700 features. Dengan menggunakan MI, 10 terms yang paling signifikan adalah *sad, thanks, miss, thank, good, love, sick, work, hate, bad .<br>\n",
    "Akurasi meningkat hingga **77.44%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "Oum0l4dmEtbr",
    "outputId": "c8a7be73-75b9-4768-fabb-ee2ec486780b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  17631\n",
      "---------------------\n",
      "sad 0.009294996835169803\n",
      "thanks 0.007328825481966443\n",
      "miss 0.005785720183408666\n",
      "love 0.005091748163164076\n",
      "sick 0.004709206870385366\n",
      "good 0.00408905897754629\n",
      "bad 0.0038255447193213162\n",
      "wish 0.003747292695550559\n",
      "thank 0.003662163464954462\n",
      "sorry 0.003574547204913292\n",
      "---------------------\n",
      "Training Classifier...\n",
      "Predicting...\n",
      "Accuracy: 0.7465181058495822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "vectorizer = CountVectorizer()\n",
    "X_training_vector = vectorizer.fit_transform(X_train_part1)\n",
    "print(\"Number of features:  %d\" % len(vectorizer.vocabulary_))\n",
    "print (\"---------------------\")\n",
    "X_test_vector = vectorizer.transform(X_test)\n",
    "\n",
    "#print features their MI score\n",
    "feature_scores = mutual_info_classif(X_training_vector, Y_train_part1)\n",
    "for score, fname in sorted(zip(feature_scores, vectorizer.get_feature_names()), reverse=True)[:10]:\n",
    "    print(fname, score)\n",
    "\n",
    "#selectKBest feature using Mutual Information and see whether it could improve the accuracy\n",
    "mic = SelectKBest(mutual_info_classif, k=700)\n",
    "X_train_best = mic.fit_transform(X_training_vector, Y_train_part1)\n",
    "X_test_best = mic.transform(X_test_vector)\n",
    "print (\"---------------------\")\n",
    "print(\"Training Classifier...\")\n",
    "clf.fit(X_train_best, Y_train_part1)\n",
    "print(\"Predicting...\")\n",
    "prediction = clf.predict(X_test_best)\n",
    "accuracy = accuracy_score(Y_test, prediction)\n",
    "print ('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment_Analysis_Feature_Selection.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
